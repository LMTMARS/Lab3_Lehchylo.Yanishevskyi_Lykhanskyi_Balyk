---
title: 'P&S-2025: Lab assignment 3'
author: "Volodymyr Lehchylo Yanishevskyi, Oleksandr Lykhanskyi, Mykola Balyk"
output:
  html_document:
    df_print: paged
---


---
Problem1, Part1 - Mykola Balyk
Problem1, Part2 - Volodymyr Lehchylo Yanishevskyi
Problem2 - Oleksandr Lykhanskyi
---

<<<<<<< HEAD
```{r setup, echo=TRUE}
# === USER: set your team id here (two-digit ordinal number from R random selection)
team_id <- 7  # <-- replace 7 with your team id
set.seed(team_id)

# true parameter
theta <- team_id/10

# packages
library(ggplot2)
library(dplyr)
library(tidyr)
options(scipen=10)
library(rmarkdown)
```

## Problem 1 — Plan

- For a range of sample sizes `n` and confidence levels `1 - alpha` we will simulate `m` repetitions of samples from Exp(rate = 1/theta).
- For each repetition we compute four confidence intervals for θ:
	1. Exact interval based on chi-square distribution (using 2 n X̄ / θ ~ χ^2_{2n}).
	2. Normal approximation using known population variance (uses true θ in formula).
	3. Interval derived by solving |θ - X̄| ≤ z θ / sqrt(n) for θ (eliminates unknown θ algebraically).
	4. Student's t-interval using sample standard deviation.
- For each method we estimate the empirical coverage probability and average length, and produce illustrative histograms.

```{r helpers, echo=TRUE}
# Helper: compute CI for one sample (x: numeric vector)
ci_methods <- function(x, alpha, theta_true=NULL){
	n <- length(x)
	xbar <- mean(x)
	s <- sd(x)
	z <- qnorm(1 - alpha/2)
	results <- list()

	# Method 1: exact via chi-square
	# 2 * (1/theta) * n * xbar ~ chi-square_{2n}  => invert to get theta
	# equivalently: 2 n xbar / theta ~ chi2_{2n}
	df <- 2*n
	L <- qchisq(alpha/2, df = df)
	U <- qchisq(1 - alpha/2, df = df)
	ci1 <- c(2*n*xbar / U, 2*n*xbar / L)
	results$exact <- ci1

	# Method 2: normal approx with known variance (uses true theta)
	if(is.null(theta_true)){
		ci2 <- c(NA, NA)
	} else {
		se_known <- theta_true / sqrt(n)
		ci2 <- c(xbar - z * se_known, xbar + z * se_known)
	}
	results$normal_knownvar <- ci2

	# Method 3: solve inequality |theta - xbar| <= z theta / sqrt(n)
	denom1 <- 1 + z / sqrt(n)
	denom2 <- 1 - z / sqrt(n)
	if(denom2 <= 0){
		ci3 <- c(NA, NA)
	} else {
		ci3 <- c(xbar / denom1, xbar / denom2)
	}
	results$algebraic <- ci3

	# Method 4: Student t using sample sd
	tval <- qt(1 - alpha/2, df = n - 1)
	se_sample <- s / sqrt(n)
	ci4 <- c(xbar - tval * se_sample, xbar + tval * se_sample)
	results$student_t <- ci4

	return(results)
}

# Single-run demonstration
demo_sample <- rexp(10, rate = 1/theta)
ci_methods(demo_sample, alpha = 0.05, theta_true = theta)
```

## Simulation: empirical coverage and average length

We run simulations to estimate coverage probability and average interval length for each method.

```{r simulate, echo=TRUE}
run_simulation <- function(n, m = 2000, alphas = c(0.1, 0.05, 0.01), theta = theta){
	out <- list()
	for(alpha in alphas){
		cover_counts <- matrix(0, nrow = m, ncol = 4)
		lengths <- matrix(NA, nrow = m, ncol = 4)
		colnames(cover_counts) <- colnames(lengths) <- c('exact','normal_knownvar','algebraic','student_t')

		for(i in seq_len(m)){
			x <- rexp(n, rate = 1/theta)
			cis <- ci_methods(x, alpha = alpha, theta_true = theta)
			for(j in seq_along(cis)){
				ci <- cis[[j]]
				if(any(is.na(ci))){
					cover_counts[i,j] <- 0
					lengths[i,j] <- NA
				} else {
					cover_counts[i,j] <- (ci[1] <= theta) && (theta <= ci[2])
					lengths[i,j] <- ci[2] - ci[1]
				}
			}
		}

		res <- tibble(
			method = c('exact','normal_knownvar','algebraic','student_t'),
			coverage = colMeans(cover_counts, na.rm = TRUE),
			avg_length = colMeans(lengths, na.rm = TRUE)
		)
		res$alpha <- alpha
		res$n <- n
		out[[as.character(alpha)]] <- res
	}
	do.call(rbind, out)
}

# Parameters: you can increase m for more precise estimates (costs more time)
n_values <- c(5, 10, 30, 100)
m <- 2000
alphas <- c(0.1, 0.05, 0.01)

sim_results <- do.call(rbind, lapply(n_values, function(nn) run_simulation(nn, m = m, alphas = alphas, theta = theta)))
sim_results <- as_tibble(sim_results)
sim_results
```

### Table of empirical coverage and average lengths

```{r table-results, echo=FALSE}
library(knitr)
kable(sim_results, digits = 4)
```

## Visualizing coverage and interval lengths

We plot coverage vs. sample size and average interval length vs. sample size for each method and alpha.

```{r plots, echo=TRUE, warning=FALSE}
sim_long <- sim_results %>%
	pivot_longer(cols = c('coverage','avg_length'), names_to = 'stat', values_to = 'value')

ggplot(sim_long, aes(x = factor(n), y = value, color = method, group = method)) +
	geom_point() + geom_line(aes(group = interaction(method, alpha))) +
	facet_grid(stat ~ alpha, scales = 'free_y', labeller = label_both) +
	labs(x = 'Sample size n', y = '', title = paste('Coverage and Average Length (theta =', theta, ', m =', m, ')')) +
	theme_minimal()
```

## Example histograms of sample means (illustrative)

```{r histograms, echo=TRUE}
set.seed(team_id)
X <- replicate(1000, mean(rexp(10, rate = 1/theta)))
qplot(X, bins = 40, xlab = 'Sample mean (n=10)', main = 'Histogram of sample mean (n=10)')
```

## Discussion and justification

- Method 1 (exact chi-square): Derivation relies on the fact that if Xi ~ Exp(rate=λ) then 2λ sum(Xi) ~ χ^2_{2n}. Using θ = 1/λ and S = n X̄ we invert quantiles of χ^2_{2n} to obtain an exact CI for θ. This interval should have coverage very close to nominal for all n.

- Method 2 (normal with known variance): Uses CLT and the known variance Var(X̄)=θ^2/n. It produces an interval X̄ ± z θ/√n which depends on θ; in simulations we use the true θ for the bound. This interval tends to have reasonable coverage for moderate/large n but can be off for small n due to skewness of the exponential.

- Method 3 (algebraic elimination): Solving |θ - X̄| ≤ z θ/√n for θ gives an interval independent of θ. This approach algebraically eliminates θ from the standard error but requires n large enough so that 1 - z/√n > 0; otherwise interval is undefined. It is an approximation derived from the same normal argument but rearranged.

- Method 4 (Student t): Replaces unknown population sd with sample sd and uses t-quantiles. For non-normal distributions (exponential is skewed) the t-approximation is not exact but often performs reasonably as n grows.

## Results summary and recommendation

From the simulation table and plots (above) we observe:

- **Coverage:** The exact chi-square method reliably attains nominal coverage across n and alpha. The normal-known-variance method has good coverage for larger n, but can be under- or over-covering for small n. The algebraic method can fail for very small n (denominator issue) and its coverage varies. The Student-t method tends to approach nominal coverage as n increases but may be biased for small n due to skewness.

- **Lengths:** The exact interval often has comparable or slightly smaller length than the conservative t-interval, especially for small to moderate n. The algebraic interval can be narrow but unstable.

Recommendation: Use Method 1 (exact chi-square) when sampling from an exponential family with known form — it is exact and reliable. If the distribution is not known or the variance is not known and n is moderate/large, Method 4 (Student t) is a practical alternative. Avoid Method 3 for very small n due to possible undefined intervals and unstable behavior.

## Conclusion

We implemented and simulated four CI procedures for θ = 1/λ of the exponential distribution. The exact chi-square based method gives the best empirical coverage and stable lengths. The normal approximation with known variance performs acceptably for larger sample sizes. The algebraic elimination method is of limited practical use for small n. The t-based interval is a reasonable general-purpose choice for moderate/large n.

---
=======
Setup chunk. 

```{r}
knitr::opts_chunk$set(echo = TRUE)
set.seed(21)
theta  <- 21 / 10
alphas <- c(0.01, 0.05, 0.1)
ns     <- c(20, 50, 100)    
M      <- 1000   
install.packages("tidyr")
```

```{r}
poisson_results <- data.frame()
for (n in ns) {
  cat("Poisson distribution, n =", n, "\n")
  
  # generating matrix of random values for P
  x <- matrix(rpois(n * M, lambda = theta), nrow = n)

  sample_mean <- colMeans(x)
  sample_sd   <- apply(x, 2, sd)
  
  for (alpha in alphas) {
    z     <- qnorm(1 - alpha/2) #z-q. for the task 2 and 3
    # t-q for the 4th
    tcrit <- qt(1 - alpha/2, df = n - 1)
    
    ## Normal approximation with knwon Var = θ
    lower2P <- sample_mean - z * sqrt(theta / n)
    upper2P <- sample_mean + z * sqrt(theta / n)
    
    # Task (a)
    cover2P  <- mean(lower2P <= theta & theta <= upper2P)
    # Task (b)
    length2P <- upper2P - lower2P
    D <- z^2 * (4 * n * sample_mean + z^2)
    lower3P <- (2 * n * sample_mean + z^2 - sqrt(D)) / (2 * n)
    upper3P <- (2 * n * sample_mean + z^2 + sqrt(D)) / (2 * n)
    
    cover3P  <- mean(lower3P <= theta & theta <= upper3P)
    length3P <- upper3P - lower3P
    
    ## via using t
    lower4P <- sample_mean - tcrit * sample_sd / sqrt(n)
    upper4P <- sample_mean + tcrit * sample_sd / sqrt(n)
    
    cover4P  <- mean(lower4P <= theta & theta <= upper4P)
    length4P <- upper4P - lower4P
    
    ## Results:
    cat("alpha =", alpha, ", confidence level =", 1 - alpha, "\n")
    
    cat("  (Method 2) Normal (known Var = θ):\n")
    cat("      fraction containing θ =", round(cover2P, 3), "\n")
    cat("      mean CI length        =", round(mean(length2P), 3), "\n")
    
    cat("  (Method 3) Solved inequality:\n")
    cat("      fraction containing θ =", round(cover3P, 3), "\n")
    cat("      mean CI length        =", round(mean(length3P), 3), "\n")
    
    cat("  (Method 4) Student t-interval:\n")
    cat("      fraction containing θ =", round(cover4P, 3), "\n")
    cat("      mean CI length        =", round(mean(length4P), 3), "\n\n")
    poisson_results <- rbind(
      poisson_results,
      data.frame(
        n = n,
        alpha = alpha,
        method = "Method 2 (normal)",
        coverage = cover2P,
        avg_length = mean(length2P)
      ),
      data.frame(
        n = n,
        alpha = alpha,
        method = "Method 3 (inequality)",
        coverage = cover3P,
        avg_length = mean(length3P)
      ),
      data.frame(
        n = n,
        alpha = alpha,
        method = "Method 4 (t-interval)",
        coverage = cover4P,
        avg_length = mean(length4P)
      )
    )
  }
}
head(poisson_results)
```


```{r}
library(ggplot2)
library(dplyr)
library(tidyr)

poisson_long <- poisson_results %>%
  mutate(alpha = factor(alpha)) %>%
  pivot_longer(cols = c("coverage", "avg_length"),
               names_to = "stat",
               values_to = "value")

ggplot(poisson_long, aes(x = factor(n), y = value, color = method)) +
  geom_point() +
  geom_line(aes(group = interaction(method, alpha))) +
  facet_grid(stat ~ alpha, scales = "free_y", labeller = label_both) +
  labs(
    x = "Sample size n",
    y = "",
    title = paste("Poisson: coverage and average CI length (theta =", theta, ", M =", M, ")")
  ) +
  theme_minimal()
```
Task (A):
In all cases the empirical coveage proportions are close to theoretical 
confidence levels 1−α:
For example:

at the 0.95 confidence level (
α=0.05), we obtain coverage of approximately 0.94–0.95;

at the 0.99 confidence level (
α=0.01), we obtain coverage of about 0.98–0.99;

at the 0.90 confidence level (
α=0.10), we obtain coverage near 0.88–0.89.

Task (B):
The same ordering for all sizes and conf levels:
Method 2 <Method 3 <Method 4
The bigger n, the smaller the intervals are.

Method 2 is unusable in real live, because it requiers a real θ for a Var.

Method 4 is the best, because it does not require knowing the true parameter, achieves accurate empirical coverage in simulations, produces intervals only slightly wider than the shortest alternative, and is simple and universally applicable.

```{r}
knitr::opts_chunk$set(echo = TRUE)
set.seed(21)
theta  <- 21 / 10
alphas <- c(0.01, 0.05, 0.1)
ns     <- c(20, 50, 100)    
M      <- 1000   
install.packages("tidyr")
```

```{r}
poisson_results <- data.frame()
for (n in ns) {
  cat("Poisson distribution, n =", n, "\n")
  
  # generating matrix of random values for P
  x <- matrix(rpois(n * M, lambda = theta), nrow = n)

  sample_mean <- colMeans(x)
  sample_sd   <- apply(x, 2, sd)
  
  for (alpha in alphas) {
    z     <- qnorm(1 - alpha/2) #z-q. for the task 2 and 3
    # t-q for the 4th
    tcrit <- qt(1 - alpha/2, df = n - 1)
    
    ## Normal approximation with knwon Var = θ
    lower2P <- sample_mean - z * sqrt(theta / n)
    upper2P <- sample_mean + z * sqrt(theta / n)
    
    # Task (a)
    cover2P  <- mean(lower2P <= theta & theta <= upper2P)
    # Task (b)
    length2P <- upper2P - lower2P
    D <- z^2 * (4 * n * sample_mean + z^2)
    lower3P <- (2 * n * sample_mean + z^2 - sqrt(D)) / (2 * n)
    upper3P <- (2 * n * sample_mean + z^2 + sqrt(D)) / (2 * n)
    
    cover3P  <- mean(lower3P <= theta & theta <= upper3P)
    length3P <- upper3P - lower3P
    
    ## via using t
    lower4P <- sample_mean - tcrit * sample_sd / sqrt(n)
    upper4P <- sample_mean + tcrit * sample_sd / sqrt(n)
    
    cover4P  <- mean(lower4P <= theta & theta <= upper4P)
    length4P <- upper4P - lower4P
    
    ## Results:
    cat("alpha =", alpha, ", confidence level =", 1 - alpha, "\n")
    
    cat("  (Method 2) Normal (known Var = θ):\n")
    cat("      fraction containing θ =", round(cover2P, 3), "\n")
    cat("      mean CI length        =", round(mean(length2P), 3), "\n")
    
    cat("  (Method 3) Solved inequality:\n")
    cat("      fraction containing θ =", round(cover3P, 3), "\n")
    cat("      mean CI length        =", round(mean(length3P), 3), "\n")
    
    cat("  (Method 4) Student t-interval:\n")
    cat("      fraction containing θ =", round(cover4P, 3), "\n")
    cat("      mean CI length        =", round(mean(length4P), 3), "\n\n")
    poisson_results <- rbind(
      poisson_results,
      data.frame(
        n = n,
        alpha = alpha,
        method = "Method 2 (normal)",
        coverage = cover2P,
        avg_length = mean(length2P)
      ),
      data.frame(
        n = n,
        alpha = alpha,
        method = "Method 3 (inequality)",
        coverage = cover3P,
        avg_length = mean(length3P)
      ),
      data.frame(
        n = n,
        alpha = alpha,
        method = "Method 4 (t-interval)",
        coverage = cover4P,
        avg_length = mean(length4P)
      )
    )
  }
}
head(poisson_results)
```


```{r}
library(ggplot2)
library(dplyr)
library(tidyr)

poisson_long <- poisson_results %>%
  mutate(alpha = factor(alpha)) %>%
  pivot_longer(cols = c("coverage", "avg_length"),
               names_to = "stat",
               values_to = "value")

ggplot(poisson_long, aes(x = factor(n), y = value, color = method)) +
  geom_point() +
  geom_line(aes(group = interaction(method, alpha))) +
  facet_grid(stat ~ alpha, scales = "free_y", labeller = label_both) +
  labs(
    x = "Sample size n",
    y = "",
    title = paste("Poisson: coverage and average CI length (theta =", theta, ", M =", M, ")")
  ) +
  theme_minimal()
```
Task (A):
In all cases the empirical coveage proportions are close to theoretical 
confidence levels 1−α:
For example:

at the 0.95 confidence level (
α=0.05), we obtain coverage of approximately 0.94–0.95;

at the 0.99 confidence level (
α=0.01), we obtain coverage of about 0.98–0.99;

at the 0.90 confidence level (
α=0.10), we obtain coverage near 0.88–0.89.

Task (B):
The same ordering for all sizes and conf levels:
Method 2 <Method 3 <Method 4
The bigger n, the smaller the intervals are.

Method 2 is unusable in real live, because it requires a real θ for a Var.

Method 4 is the best, because it does not require knowing the true parameter, achieves accurate empirical coverage in simulations, produces intervals only slightly wider than the shortest alternative, and is simple and universally applicable.

# Initializing dataset

```{r}
set.seed(21)
n <- 100
mu <- 10
sigma_pow_2 <- 4
sigma <- sqrt(sigma_pow_2)
dataset <- rnorm(n, mean = mu, sd = sigma)
head(dataset)
```

# Write the code to find the variance for the dataset

```{r}
cat("Population Mean (mu):", mu, "\n")
cat("Population Variance (sigma_pow_2):", sigma_pow_2, "\n")
s_mean <- mean(dataset)
sample_variance <- var(dataset)
cat("Sample Mean:", s_mean, "\n")
cat("Sample Variance:", sample_variance, "\n")
```

# Find σ²ₙ and σ²ₙ₋₁ for n = 10, n = 50, n = 100, n = 1000

```{r}
n_arr <- c(10, 50, 100, 1000)

results <- data.frame(n = n_arr, a = NA, b = NA)

for (i in 1:length(n_arr)) {
  n <- n_arr[i]
  d_set <- rnorm(n, mean = mu, sd = sigma)
  s_mean <- mean(d_set)
  square_sum <- sum((d_set - s_mean)^2)
  a <- square_sum / n
  b <- square_sum / (n - 1)
  results$a[i] <- a
  results$b[i] <- b
}

print(results)
```

# Find the biases Bias(σ²ₙ) = E(σ²ₙ) − σ² and Bias(σ²ₙ₋₁) = E(σ²ₙ₋₁) − σ²

```{r}
iterations <- 10000
n_arr <- c(10, 50, 100, 1000)
res <- data.frame(
  n = n_arr,
  E_var_a = NA,
  E_var_b = NA,
  B_var_a = NA,
  B_var_b = NA
)

for (i in 1:length(n_arr)) {
  n <- n_arr[i]
  
  sigma_a_vec <- numeric(iterations)
  sigma_b_vec <- numeric(iterations)
  
  for (j in 1:iterations) {
    d_set <- rnorm(n, mean = mu, sd = sigma)
    s_mean <- mean(d_set)
    square_sum <- sum((d_set - s_mean)^2)
    
    sigma_a_vec[j] <- square_sum / n
    sigma_b_vec[j] <- square_sum / (n - 1)
  }
  
  E_var_a <- mean(sigma_a_vec)
  E_var_b <- mean(sigma_b_vec)
  
  B_var_a <- E_var_a - sigma_pow_2
  B_var_b <- E_var_b - sigma_pow_2
  
  res$E_var_a[i] <- E_var_a
  res$E_var_b[i] <- E_var_b
  res$B_var_a[i] <- B_var_a
  res$B_var_b[i] <- B_var_b
}

cat("True Population Variance (σ²):", sigma_pow_2, "\n\n")
print("Bias Analysis Results:")
print(res)
```

# Derive analytically the expected value of each estimator

## Estimator σ²ₙ (Type a)

$$E\left[\sum_{i=1}^{n}(X_i - \bar{X})^2\right] = (n-1)\sigma^2$$

$$E(\sigma^2_n) = E\left[\frac{1}{n}\sum_{i=1}^{n}(X_i - \bar{X})^2\right]$$

$$= \frac{1}{n} \times E\left[\sum_{i=1}^{n}(X_i - \bar{X})^2\right]$$

$$= \frac{1}{n} \times (n-1)\sigma^2$$

$$= \frac{n-1}{n}\sigma^2$$

**Bias:**

$$\text{Bias}(\sigma^2_n) = E(\sigma^2_n) - \sigma^2$$

$$= \frac{n-1}{n}\sigma^2 - \sigma^2$$

$$= -\frac{\sigma^2}{n}$$

```{r}
analyt_a <- data.frame(
  n = n_arr,
  E_var_a_analyt = ((n_arr - 1) / n_arr) * sigma_pow_2,
  B_var_a_analyt = -sigma_pow_2 / n_arr
)

cat("Results for Type a variance:\n")
print(analyt_a)
cat("\n")
```

## Estimator σ²ₙ₋₁ (Type b)

$$E\left[\sum_{i=1}^{n}(X_i - \bar{X})^2\right] = (n-1)\sigma^2$$

$$E(\sigma^2_{n-1}) = E\left[\frac{1}{n-1}\sum_{i=1}^{n}(X_i - \bar{X})^2\right]$$

$$= \frac{1}{n-1} \times E\left[\sum_{i=1}^{n}(X_i - \bar{X})^2\right]$$

$$= \frac{1}{n-1} \times (n-1)\sigma^2$$

$$= \sigma^2$$

**Bias:**

$$\text{Bias}(\sigma^2_{n-1}) = E(\sigma^2_{n-1}) - \sigma^2$$

$$= \sigma^2 - \sigma^2$$

$$= 0$$

```{r}
analyt_b <- data.frame(
  n = n_arr,
  E_var_b_analyt = rep(sigma_pow_2, length(n_arr)),
  B_var_b_analyt = rep(0, length(n_arr))
)

cat("Results for Type b variance:\n")
print(analyt_b)
cat("\n")
```

## Summary of Analytical Results

```{r}
res_analyt <- data.frame(
  n = n_arr,
  E_var_a_analyt = ((n_arr - 1) / n_arr) * sigma_pow_2,
  B_var_a_analyt = -sigma_pow_2 / n_arr,
  E_var_b_analyt = rep(sigma_pow_2, length(n_arr)),
  B_var_b_analyt = rep(0, length(n_arr))
)

print(res_analyt)
cat("\n")
```

## Comparison: Analytical vs Simulation

```{r}
comparison <- data.frame(
  n = n_arr,
  E_var_a_analyt = res_analyt$E_var_a_analyt,
  E_var_a_simulation = res$E_var_a,
  B_var_a_analyt = res_analyt$B_var_a_analyt,
  B_var_a_simulation = res$B_var_a,
  E_var_b_analyt = res_analyt$E_var_b_analyt,
  E_var_b_simulation = res$E_var_b,
  B_var_b_analyt = res_analyt$B_var_b_analyt,
  B_var_b_simulation = res$B_var_b
)

print(comparison)
cat("\n")
```

# Using the expected values found above, show mathematically what of the above two estimators are unbiased

## Checking σ²ₙ (Type a)

```{r}
unbiased_check_a <- data.frame(
  n = n_arr,
  E_var_a = ((n_arr - 1) / n_arr) * sigma_pow_2,
  sigma_sq = rep(sigma_pow_2, length(n_arr)),
  is_equal = rep("NO", length(n_arr))
)

print(unbiased_check_a)
cat("\n")
```

**Mathematical proof:**

$$E(\sigma^2_n) = \frac{n-1}{n}\sigma^2$$

$$E(\sigma^2_n) = \sigma^2 - \frac{\sigma^2}{n}$$

$$E(\sigma^2_n) \neq \sigma^2 \quad \text{(since } \frac{\sigma^2}{n} > 0\text{)}$$

**Conclusion:** $\sigma^2_n$ is **biased**

**Note:** As $n \to \infty$, $\text{Bias}(\sigma^2_n) \to 0$, so $\sigma^2_n$ is **asymptotically unbiased**

## Checking σ²ₙ₋₁ (Type b)

```{r}
unbiased_check_b <- data.frame(
  n = n_arr,
  E_var_b = rep(sigma_pow_2, length(n_arr)),
  sigma_sq = rep(sigma_pow_2, length(n_arr)),
  is_equal = rep("YES", length(n_arr))
)

print(unbiased_check_b)
cat("\n")
```

**Mathematical proof:**

$$E(\sigma^2_{n-1}) = E\left[\frac{1}{n-1}\sum_{i=1}^{n}(X_i - \bar{X})^2\right]$$

$$= \frac{1}{n-1} \times E\left[\sum_{i=1}^{n}(X_i - \bar{X})^2\right]$$

$$= \frac{1}{n-1} \times (n-1)\sigma^2$$

$$= \sigma^2$$

**Therefore:**

$$E(\sigma^2_{n-1}) = \sigma^2$$

**Conclusion:** $\sigma^2_{n-1}$ is **unbiased**

$$\text{Bias}(\sigma^2_{n-1}) = E(\sigma^2_{n-1}) - \sigma^2 = 0$$
We examined two variances: σ²ₙ (dividing by n) and σ²ₙ₋₁ (dividing by n-1), comparing their theoretical values with empirical values.

The empirical results were close to analytical ones across all sample sizes tested (n = 10, 50, 100, 1000).

The estimator σ²ₙ consistently underestimated the true population variance (σ² = 4) with a negative bias of -σ²/n, which decreased as sample size increased, demonstrating asymptotic unbiasedness.

σ²ₙ₋₁ produced unbiased estimates across all sample sizes with bias values near zero.

Thus we confirmed that E(σ²ₙ) = ((n-1)/n)σ² while E(σ²ₙ₋₁) = σ², proving that dividing by n-1 compensates for using the sample mean instead of the population mean.

For practical applications σ²ₙ₋₁ should be preferred as it provides unbiased estimates regardless of sample size.